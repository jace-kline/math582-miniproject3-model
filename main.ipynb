{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn.svm import SVC as SVC_sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to prep a given Pandas dataset\n",
    "\n",
    "# This function performs the following:\n",
    "# - drops NaN/empty values\n",
    "# - maps classifiers values -1 or +1 (e.g [\"No\", \"Yes\"] -> [-1, 1])\n",
    "# - separates feature columns from the classifier column\n",
    "# - splits the data into training and testing sets\n",
    "# returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_data(data, classifier_column_name, classifier_vals=None, train_size=0.75):\n",
    "    \n",
    "    # map each binary classifier value to either 1 or -1\n",
    "    if classifier_vals is not None:\n",
    "        if len(classifier_vals) != 2:\n",
    "            raise ValueError(\"classifier_vals argument must be length 2 (binary classifier)\")\n",
    "        else:\n",
    "            data[classifier_column_name] = \\\n",
    "                data[classifier_column_name].apply(lambda b: -1 if b == classifier_vals[0] else 1)\n",
    "\n",
    "    # Drop NaN rows\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # separate the features from the classifications\n",
    "    colnames = data.columns.tolist()\n",
    "    feature_column_names = list(filter(lambda colname: colname != classifier_column_name, colnames))\n",
    "\n",
    "    X = data[feature_column_names]\n",
    "    y = data[classifier_column_name]\n",
    "\n",
    "    # split the data into training and testing data\n",
    "    datasets = train_test_split(X, y, train_size=train_size)\n",
    "\n",
    "    # map all the training data into numpy arrays\n",
    "    X_train, X_test, y_train, y_test = list(map(lambda s: s.to_numpy(), datasets))\n",
    "\n",
    "    # return the training and testing data\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that exposes common kernel functions\n",
    "\n",
    "def annotate(name, params=None):\n",
    "    def decorator(f):\n",
    "        f.name = name\n",
    "        f.params = params\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Kernel(object):\n",
    "    \"\"\"\n",
    "    Definitions of some common kernel functions.\n",
    "    Call each of these functions with their respective kernel parameters to obtain a function object that acts on two training data points x, y.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        @annotate(name='Linear')\n",
    "        def f(x, y):\n",
    "            return np.inner(x, y)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        @annotate(name='Gaussian', params=dict(sigma=sigma))\n",
    "        def f(x, y):\n",
    "            exponent = -np.sqrt(la.norm(x-y) ** 2 / (2 * sigma ** 2))\n",
    "            return np.exp(exponent)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial(dimension, offset):\n",
    "        @annotate(name='Polynomial', params=dict(dimension=dimension, offset=offset))\n",
    "        def f(x, y):\n",
    "            return (offset + np.dot(x, y)) ** dimension\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def inhomogenous_polynomial(dimension):\n",
    "        return Kernel.polynomial(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def homogenous_polynomial(dimension):\n",
    "        return Kernel.polynomial(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        @annotate(name='Hyperbolic Tangent', params=dict(kappa=kappa, c=c))\n",
    "        def f(x, y):\n",
    "            return np.tanh(kappa * np.dot(x, y) + c)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def rbf(gamma):\n",
    "        @annotate(name='RBF', params=dict(gamma=gamma))\n",
    "        def f(x, y):\n",
    "            return (np.exp(-gamma*(np.linalg.norm(x - y))**2))\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model group's implementation of the Support Vector Classifier\n",
    "# Computes the SVM via the dual optimization problem, a quadratic program\n",
    "class SVC(object):\n",
    "    \"\"\"\n",
    "    Our definition of an SVC Classifier Model.\n",
    "    \"\"\"\n",
    "    # constructor\n",
    "    def __init__(self, kernel=Kernel.linear(), C=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "\n",
    "    # helper function to build kernel matrix\n",
    "    def _build_k(self, X):\n",
    "        \"\"\"\n",
    "        build_k generates a kernel to use inside of an SVM calculation\n",
    "        X: Training data for our calculations\n",
    "        kernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "        poly_power: An optional parameter to define to what degree the polynomial should be calculated\n",
    "        gamma: An optional parameter that defines how far the influence of a single training example reaches\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        kernel = self.kernel\n",
    "        N = X.shape[0]\n",
    "        K = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            x_i = X[i]\n",
    "            for j in range(N):\n",
    "                x_j = X[j]\n",
    "\n",
    "                K[i][j] = kernel(x_i, x_j)\n",
    "\n",
    "        return K\n",
    "\n",
    "    # solve the dual SVM problem with given training data\n",
    "    # find alphas, w, and b for use in predictions\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        SVM will calculate the weight and bias using the SVM quadratic method (soft margin)\n",
    "        X: Training data used for calculations \n",
    "        y: results of training data\n",
    "        kernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "        C: Trades off misclassification of training examples against simplicity of the decision surface\n",
    "        :return: weight, bias, and alphas matrix\n",
    "\n",
    "        Help used: https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work/353605#353605\n",
    "        \"\"\"\n",
    "        # map member variables / methods to shorter aliases\n",
    "        C = self.C\n",
    "        kernel = self.kernel\n",
    "        build_k = self._build_k\n",
    "\n",
    "        # Grabs shape of our training data\n",
    "        m, _ = X.shape\n",
    "\n",
    "        # Make sure y values are floats and within -1 == y == 1\n",
    "        y = y.reshape(-1,1) * 1.\n",
    "\n",
    "        # Calculate our kernel\n",
    "        K = build_k(X)\n",
    "\n",
    "        # Compute \n",
    "        H = np.matmul(y,y.T) * K * 1.\n",
    "\n",
    "        #Converting into cvxopt format - as previously\n",
    "        P = cvxopt_matrix(H)\n",
    "        q = cvxopt_matrix(-np.ones((m, 1)))\n",
    "        G = cvxopt_matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "        h = cvxopt_matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "        A = cvxopt_matrix(y.reshape(1, -1))\n",
    "        b = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "        # disable outputs from solver\n",
    "        solvers.options['qp'] = dict(msg_lev='QP_MSG_OFF')\n",
    "        solvers.options['show_progress'] = False\n",
    "\n",
    "        # Run solver\n",
    "        sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.array(sol['x'])\n",
    "\n",
    "        # Calculating w, b\n",
    "        w = ((y * alphas).T @ X).reshape(-1,1).flatten()\n",
    "        S = (alphas > 1e-4).flatten()\n",
    "\n",
    "        sv = X[S]\n",
    "        sv_y = y[S]\n",
    "        alphas = alphas[S]\n",
    "        b = sv_y - np.sum(build_k(sv) * alphas * sv_y, axis=0)\n",
    "        b = [np.sum(b) / b.size]\n",
    "\n",
    "        # set the member variables for building the predict method\n",
    "        self.alphas = alphas.flatten()\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "        ### build the prediction function ###\n",
    "\n",
    "        # classifies a single sample as +1 or -1\n",
    "        def classify_sample(x):\n",
    "            return (1 if np.inner(w, x) + b >= 0 else -1)\n",
    "\n",
    "        # classifies multiple samples as +1 or -1 -> outputs array\n",
    "        def classify_samples(X_test):\n",
    "            return np.apply_along_axis(classify_sample, 1, X_test)\n",
    "\n",
    "        def classify(X):\n",
    "            if X.shape == w.shape:\n",
    "                return classify_sample(X)\n",
    "            elif len(X.shape) > 1 and X[0].shape == w.shape:\n",
    "                return classify_samples(X)\n",
    "            else:\n",
    "                raise Exception(\"Invalid test data shape. Either input an array (single sample) or a 2d array (multiple samples).\")\n",
    "\n",
    "        self._predict = classify\n",
    "\n",
    "    # predict the binary classifications {-1,1} of test sample arrays in X\n",
    "    # outputs a vector (or value) of classifications in {-1,1}\n",
    "    # requires that fit() was called previously\n",
    "    def predict(self, X):\n",
    "        # if data not fitted -> predict function not valid\n",
    "        if self._predict is None:\n",
    "            raise Exception(\"SVC Model must be fitted before prediction. Utilize fit() method with training data.\")\n",
    "\n",
    "        # otherwise, return the prediction results\n",
    "        # can be given a single sample (shape equal to w)\n",
    "        # or multiple samples (a 2d array with elements that have shape equal to w)\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to evaluate classifier performance over test data\n",
    "\n",
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    numtests = X_test.shape[0]\n",
    "    results = classifier(X_test) == y_test.flatten()\n",
    "    numcorrect = np.count_nonzero(results)\n",
    "    successrate = numcorrect / numtests\n",
    "    return successrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "\n",
    "In this section, we demonstrate our SVC model on a sample data set using the linear and polynomial kernels. This section serves to demonstrate how to use our SVC class. We strove to expose the same methods (i.e. fit() and predict()) as are provided by the Scikit Learn implementation of SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a data set into Pandas DataFrame\n",
    "data = pd.read_csv('./data/test-data/test_data.csv')\n",
    "\n",
    "# Prepare our data for SVC\n",
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    data=data, # the Pandas DataFrame\n",
    "    classifier_column_name=\"success\", # classification column name\n",
    "    classifier_vals=[0.0, 1.0], # classification values -> mapped to {-1, 1}\n",
    "    train_size=0.75 # the proportion of data to allocate for training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to for running a comparison test\n",
    "# between our model and Scikit Learn's model\n",
    "\n",
    "def fit_eval_compare(kernel, C, kernel_name=None, kernel_params=None):\n",
    "    # Our SVC model -> fit and evaluate\n",
    "    our_svc = SVC(kernel=kernel, C=C)\n",
    "    our_svc.fit(X_train, y_train)\n",
    "    our_result = evaluate_classifier(our_svc.predict, X_test, y_test)\n",
    "\n",
    "    # SciKit Learn SVC model -> fit and evaluate\n",
    "    sklearn_svc = SVC_sklearn(kernel=kernel, C=C)\n",
    "    sklearn_svc.fit(X_train, y_train)\n",
    "    sklearn_result = evaluate_classifier(sklearn_svc.predict, X_test, y_test)\n",
    "\n",
    "    kernel_name = kernel_name or kernel.name or None\n",
    "    kernel_params = kernel_params or kernel.params or dict()\n",
    "\n",
    "    # Return results\n",
    "    if kernel_name: print(f\"Kernel: {kernel_name}\")\n",
    "    if kernel_params: print(f\"Kernel parameters: {kernel_params}\")\n",
    "    print(f\"Our result: {our_result}\")\n",
    "    print(f\"SKLearn result: {sklearn_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our result: 0.8533333333333334\n",
      "SKLearn result: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Demonstration 1: Linear Kernel\n",
    "# Define test parameters for the SVC\n",
    "# use a linear kernel function and C=1.0\n",
    "kernel = Kernel.linear()\n",
    "C = 1.0\n",
    "\n",
    "# Our SVC model\n",
    "our_svc = SVC(kernel=kernel, C=C)\n",
    "our_svc.fit(X_train, y_train)\n",
    "our_result = evaluate_classifier(our_svc.predict, X_test, y_test)\n",
    "\n",
    "# SciKit Learn SVC model\n",
    "sklearn_svc = SVC_sklearn(kernel=kernel, C=C)\n",
    "sklearn_svc.fit(X_train, y_train)\n",
    "sklearn_result = evaluate_classifier(sklearn_svc.predict, X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Our result: {our_result}\")\n",
    "print(f\"SKLearn result: {sklearn_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: Linear\n",
      "Our result: 0.8533333333333334\n",
      "SKLearn result: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "fit_eval_compare(kernel, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (222,2) and (222,2) not aligned: 2 (dim 1) != 222 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12995/2693142922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_eval_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12995/2674617559.py\u001b[0m in \u001b[0;36mfit_eval_compare\u001b[0;34m(kernel, C, kernel_name, kernel_params)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# SciKit Learn SVC model -> fit and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msklearn_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msklearn_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0msklearn_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installs/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installs/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# TODO: add keyword copy to copy on demand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__Xfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installs/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_compute_kernel\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# in the case of precomputed kernel given as a function, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# have to compute explicitly the kernel matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__Xfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12995/2638661417.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Polynomial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (222,2) and (222,2) not aligned: 2 (dim 1) != 222 (dim 0)"
     ]
    }
   ],
   "source": [
    "fit_eval_compare(Kernel.polynomial(3, 0.0), C)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd90b7313dd068aed6076c54fbc0f4176173242dcb889e69e2c3450f3e374009"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
