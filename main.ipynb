{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn.svm import SVC as SVC_sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to prep a given Pandas dataset\n",
    "\n",
    "# This function performs the following:\n",
    "# - drops NaN/empty values\n",
    "# - maps classifiers values -1 or +1 (e.g [\"No\", \"Yes\"] -> [-1, 1])\n",
    "# - separates feature columns from the classifier column\n",
    "# - splits the data into training and testing sets\n",
    "# returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_data(data, classifier_column_name, classifier_vals, train_size=0.75):\n",
    "\n",
    "    if len(classifier_vals) != 2:\n",
    "        raise ValueError(\"classifier_vals argument must be length 2 (binary classifier)\")\n",
    "    \n",
    "    # map each binary classifier value to either 1 or -1\n",
    "    data[classifier_column_name] = data[classifier_column_name].apply(lambda b: -1 if b == classifier_vals[0] else 1)\n",
    "\n",
    "    # separate the features from the classifications\n",
    "    colnames = data.columns.tolist()\n",
    "    feature_column_names = list(filter(lambda colname: colname != classifier_column_name, colnames))\n",
    "\n",
    "    xs = data[feature_column_names]\n",
    "    ys = data[classifier_column_name]\n",
    "\n",
    "    # split the data into training and testing data\n",
    "    datasets = train_test_split(xs, ys, train_size=train_size)\n",
    "\n",
    "    # map all the training data into numpy arrays\n",
    "    X_train, X_test, y_train, y_test = list(map(lambda s: s.to_numpy(), datasets))\n",
    "\n",
    "    # return the training and testing data\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that exposes common kernel functions\n",
    "\n",
    "class Kernel(object):\n",
    "    \"\"\"\n",
    "    Definitions of some common kernel functions.\n",
    "    Call each of these functions with their respective kernel parameters to obtain a function object that acts on two training data points x, y.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        def f(x, y):\n",
    "            return np.inner(x, y)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        def f(x, y):\n",
    "            exponent = -np.sqrt(la.norm(x-y) ** 2 / (2 * sigma ** 2))\n",
    "            return np.exp(exponent)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def _polykernel(dimension, offset):\n",
    "        def f(x, y):\n",
    "            return (offset + np.dot(x, y)) ** dimension\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def inhomogenous_polynomial(dimension):\n",
    "        return Kernel._polykernel(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def homogenous_polynomial(dimension):\n",
    "        return Kernel._polykernel(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        def f(x, y):\n",
    "            return np.tanh(kappa * np.dot(x, y) + c)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def rbf(gamma):\n",
    "        def f(x, y):\n",
    "            return (np.exp(-gamma*(np.linalg.norm(x - y))**2))\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model group's implementation of the Support Vector Classifier\n",
    "# Computes the SVM via the dual optimization problem, a quadratic program\n",
    "class SVC(object):\n",
    "    \"\"\"\n",
    "    Our definition of an SVC Classifier Model.\n",
    "    \"\"\"\n",
    "    # constructor\n",
    "    def __init__(self, kernel=Kernel.linear(), C=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "\n",
    "    # helper function to build kernel matrix\n",
    "    def _build_k(self, X):\n",
    "        \"\"\"\n",
    "        build_k generates a kernel to use inside of an SVM calculation\n",
    "        X: Training data for our calculations\n",
    "        kernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "        poly_power: An optional parameter to define to what degree the polynomial should be calculated\n",
    "        gamma: An optional parameter that defines how far the influence of a single training example reaches\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        kernel = self.kernel\n",
    "        N = X.shape[0]\n",
    "        K = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            x_i = X[i]\n",
    "            for j in range(N):\n",
    "                x_j = X[j]\n",
    "\n",
    "                K[i][j] = kernel(x_i, x_j)\n",
    "\n",
    "        return K\n",
    "\n",
    "    # solve the dual SVM problem with given training data\n",
    "    # find alphas, w, and b for use in predictions\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        SVM will calculate the weight and bias using the SVM quadratic method (soft margin)\n",
    "        X: Training data used for calculations \n",
    "        y: results of training data\n",
    "        kernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "        C: Trades off misclassification of training examples against simplicity of the decision surface\n",
    "        :return: weight, bias, and alphas matrix\n",
    "\n",
    "        Help used: https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work/353605#353605\n",
    "        \"\"\"\n",
    "        # map member variables / methods to shorter aliases\n",
    "        C = self.C\n",
    "        kernel = self.kernel\n",
    "        build_k = self._build_k\n",
    "\n",
    "        # Grabs shape of our training data\n",
    "        m, _ = X.shape\n",
    "\n",
    "        # Make sure y values are floats and within -1 == y == 1\n",
    "        y = y.reshape(-1,1) * 1.\n",
    "\n",
    "        # Calculate our kernel\n",
    "        K = build_k(X)\n",
    "\n",
    "        # Compute \n",
    "        H = np.matmul(y,y.T) * K * 1.\n",
    "\n",
    "        #Converting into cvxopt format - as previously\n",
    "        P = cvxopt_matrix(H)\n",
    "        q = cvxopt_matrix(-np.ones((m, 1)))\n",
    "        G = cvxopt_matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "        h = cvxopt_matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "        A = cvxopt_matrix(y.reshape(1, -1))\n",
    "        b = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "        #Run solver\n",
    "        sol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.array(sol['x'])\n",
    "\n",
    "        # Calculating w, b\n",
    "        w = ((y * alphas).T @ X).reshape(-1,1).flatten()\n",
    "        S = (alphas > 1e-4).flatten()\n",
    "\n",
    "        sv = X[S]\n",
    "        sv_y = y[S]\n",
    "        alphas = alphas[S]\n",
    "        b = sv_y - np.sum(build_k(sv) * alphas * sv_y, axis=0)\n",
    "        b = [np.sum(b) / b.size]\n",
    "\n",
    "        # set the member variables for building the predict method\n",
    "        self.alphas = alphas\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "        ### build the prediction function ###\n",
    "\n",
    "        # classifies a single sample as +1 or -1\n",
    "        def classify_sample(x):\n",
    "            return (1 if np.inner(w, x) + b >= 0 else -1)\n",
    "\n",
    "        # classifies multiple samples as +1 or -1 -> outputs array\n",
    "        def classify_samples(X_test):\n",
    "            return np.apply_along_axis(classify_sample, 1, X_test)\n",
    "\n",
    "        def classify(X):\n",
    "            if X.shape == w.shape:\n",
    "                return classify_sample(X)\n",
    "            elif len(X.shape) > 1 and X[0].shape == w.shape:\n",
    "                return classify_samples(X)\n",
    "            else:\n",
    "                raise Exception(\"Invalid test data shape. Either input an array (single sample) or a 2d array (multiple samples).\")\n",
    "\n",
    "        self._predict = classify\n",
    "\n",
    "    # predict the binary classifications {-1,1} of test sample arrays in X\n",
    "    # outputs a vector (or value) of classifications in {-1,1}\n",
    "    # requires that fit() was called previously\n",
    "    def predict(self, X):\n",
    "        # if data not fitted -> predict function not valid\n",
    "        if self._predict is None:\n",
    "            raise Exception(\"SVC Model must be fitted before prediction. Utilize fit() method with training data.\")\n",
    "\n",
    "        # otherwise, return the prediction results\n",
    "        # can be given a single sample (shape equal to w)\n",
    "        # or multiple samples (a 2d array with elements that have shape equal to w)\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to evaluate classifier performance over test data\n",
    "\n",
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    numtests = X_test.shape[0]\n",
    "    results = classifier(X_test) == y_test.flatten()\n",
    "    numcorrect = np.count_nonzero(results)\n",
    "    successrate = numcorrect / numtests\n",
    "    return successrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a data set into Pandas DataFrame\n",
    "data = pd.read_csv('./data/test-data/test_data.csv')\n",
    "\n",
    "# Prepare our data for SVC\n",
    "X_train, X_test, y_train, y_test = prep_data(\n",
    "    data=data, # the Pandas DataFrame\n",
    "    classifier_column_name=\"success\", # classification column name\n",
    "    classifier_vals=[0.0, 1.0], # classification values -> mapped to {-1, 1}\n",
    "    train_size=0.75 # the proportion of data to allocate for training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.4037e+01 -4.9358e+02  2e+03  3e+00  9e-12\n",
      " 1: -6.3788e+01 -3.0763e+02  4e+02  3e-01  8e-12\n",
      " 2: -5.7724e+01 -1.0874e+02  7e+01  4e-02  6e-12\n",
      " 3: -6.2428e+01 -7.4474e+01  1e+01  8e-03  5e-12\n",
      " 4: -6.4524e+01 -6.9819e+01  6e+00  3e-03  6e-12\n",
      " 5: -6.5336e+01 -6.7563e+01  2e+00  7e-04  6e-12\n",
      " 6: -6.5683e+01 -6.7145e+01  2e+00  2e-04  6e-12\n",
      " 7: -6.5982e+01 -6.6667e+01  7e-01  6e-05  6e-12\n",
      " 8: -6.6089e+01 -6.6491e+01  4e-01  2e-05  6e-12\n",
      " 9: -6.6178e+01 -6.6369e+01  2e-01  7e-06  6e-12\n",
      "10: -6.6230e+01 -6.6304e+01  7e-02  3e-16  7e-12\n",
      "11: -6.6261e+01 -6.6269e+01  8e-03  2e-15  6e-12\n",
      "12: -6.6265e+01 -6.6265e+01  2e-04  2e-16  7e-12\n",
      "13: -6.6265e+01 -6.6265e+01  2e-06  2e-16  7e-12\n",
      "Optimal solution found.\n",
      "Our result: 0.8933333333333333\n",
      "SKLearn result: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters for the SVC\n",
    "# use a linear kernel function and C=1.0\n",
    "kernel = Kernel.linear()\n",
    "C = 1.0\n",
    "\n",
    "# Our SVC model\n",
    "our_svc = SVC(kernel=kernel, C=C)\n",
    "our_svc.fit(X_train, y_train)\n",
    "our_result = evaluate_classifier(our_svc.predict, X_test, y_test)\n",
    "\n",
    "# SciKit Learn SVC model\n",
    "sklearn_svc = SVC_sklearn(kernel=kernel, C=C)\n",
    "sklearn_svc.fit(X_train, y_train)\n",
    "sklearn_result = evaluate_classifier(sklearn_svc.predict, X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Our result: {our_result}\")\n",
    "print(f\"SKLearn result: {sklearn_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd90b7313dd068aed6076c54fbc0f4176173242dcb889e69e2c3450f3e374009"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
