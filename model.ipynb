{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 582 Miniproject 3 - Model Development\n",
    "\n",
    "The purpose of this notebook is implment dual SVM convex quadratic optimization for the purposes of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn.svm import SVC as SVC_sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel function definitions\n",
    "\n",
    "def polynomial_kernel (x_i, x_j, d):\n",
    "\t\"\"\"\n",
    "\tpolynomial_kernel generates a kernel for SVM in a polynomial format\n",
    "\n",
    "\tx_i: ith column to chose from\n",
    "\tx_j: jth row to choose from \n",
    "\td: polynomial degree\n",
    "\n",
    "\t:return: kernel values for the i, jth value\n",
    "\t\"\"\"\n",
    "\treturn (np.dot(x_i, x_j) + 1/2)**d\n",
    "\n",
    "def linear_kernel (x_i, x_j):\n",
    "\t\"\"\"\n",
    "\tlinear_kernel generates a kernel for SVM in a linear format\n",
    "\n",
    "\tx_i: ith column to chose from\n",
    "\tx_j: jth row to choose from \n",
    "\t:return: kernel values for the i, jth value\n",
    "\t\"\"\"\n",
    "\treturn (np.dot(x_i, x_j))\n",
    "\n",
    "def rbf_kernel (x_i, x_j,gamma=1):\n",
    "\t\"\"\"\n",
    "\trbf_kernel generates a kernel for SVM in a radial format\n",
    "\n",
    "\tx_i: ith column to chose from\n",
    "\tx_j: jth row to choose from \n",
    "\tgamma: defines how far the influence of a single training example reaches\n",
    "\t:return: kernel values for the i, jth value\n",
    "\t\"\"\"\n",
    "\treturn (np.exp(-gamma*(np.linalg.norm(x_j - x_i))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the kernel matrix from a kernel spec and data\n",
    "\n",
    "def build_k (X, kernel_type='linear_kernel', poly_power=3, gamma=1):\n",
    "\t\"\"\"\n",
    "\tbuild_k generates a kernel to use inside of an SVM calculation\n",
    "\tX: Training data for our calculations\n",
    "\tkernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "\tpoly_power: An optional parameter to define to what degree the polynomial should be calculated\n",
    "\tgamma: An optional parameter that defines how far the influence of a single training example reaches\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tN = X.shape[0]\n",
    "\tK = np.zeros((N, N))\n",
    "\tfor i in range(X.shape[0]):\n",
    "\t\tx_i = X[i]\n",
    "\t\tfor j in range(X.shape[0]):\n",
    "\t\t\tx_j = X[j]\n",
    "\n",
    "\t\t\tif kernel_type == 'linear_kernel':\n",
    "\t\t\t\tK[i][j] = linear_kernel(x_i, x_j)\n",
    "\n",
    "\t\t\telif kernel_type == 'polynomial_kernel':\n",
    "\t\t\t\tK[i][j] = polynomial_kernel(x_i, x_j, poly_power)\n",
    "\n",
    "\t\t\telif kernel_type == 'rbf_kernel':\n",
    "\t\t\t\tK[i][j] = rbf_kernel(x_i, x_j, gamma)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError('Use kernal type polynomial_kernel, linear_kernel or rbf_kernel') \n",
    "\n",
    "\treturn K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This outputs weights, biases, and lagrange multipliers of our SVM Classifier\n",
    "def SVM(X, y, kernel_type='linear_kernel', C=10):\n",
    "\t\"\"\"\n",
    "\tSVM will calculate the weight and bias using the SVM quadratic method (soft margin)\n",
    "\tX: Training data used for calculations \n",
    "\ty: results of training data\n",
    "\tkernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "\tC: Trades off misclassification of training examples against simplicity of the decision surface\n",
    "\t:return: weight, bias, and alphas matrix\n",
    "\n",
    "\tHelp used: https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work/353605#353605\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grabs shape of our training data\n",
    "\tm, _ = X.shape\n",
    "\n",
    "\t# Make sure y values are floats and within -1 == y == 1\n",
    "\ty = y.reshape(-1,1) * 1.\n",
    "\n",
    "\t# Calculate our kernel\n",
    "\tK = build_k(X, kernel_type=kernel_type)\n",
    "\n",
    "\t# Compute \n",
    "\tH = np.matmul(y,y.T) * K * 1.\n",
    "\n",
    "\t#Converting into cvxopt format - as previously\n",
    "\tP = cvxopt_matrix(H)\n",
    "\tq = cvxopt_matrix(-np.ones((m, 1)))\n",
    "\tG = cvxopt_matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "\th = cvxopt_matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "\tA = cvxopt_matrix(y.reshape(1, -1))\n",
    "\tb = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "\t#Run solver\n",
    "\tsol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "\talphas = np.array(sol['x'])\n",
    "\n",
    "\t# Calculating w, b\n",
    "\tw = ((y * alphas).T @ X).flatten() #.reshape(-1,1).\n",
    "\tS = (alphas > 1e-4).flatten()\n",
    "\n",
    "\tsv = X[S]\n",
    "\tsv_y = y[S]\n",
    "\talphas = alphas[S]\n",
    "\tb = sv_y - np.sum(build_k(sv) * alphas * sv_y, axis=0)\n",
    "\tb = [np.sum(b) / b.size]\n",
    "\n",
    "\treturn w, b, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def SVM_classifier(X_train, y_train):\n",
    "    w, b, alphas = SVM(X_train, y_train)\n",
    "\n",
    "    # classifies a single sample as +1 or -1\n",
    "    def classify_sample(x):\n",
    "        return (1 if np.inner(w, x) + b >= 0 else -1)\n",
    "\n",
    "    # classifies multiple samples as +1 or -1 -> outputs array\n",
    "    def classify_samples(X_test):\n",
    "        return np.apply_along_axis(classify_sample, 1, X_test)\n",
    "\n",
    "    def classify(X):\n",
    "        if X.shape == w.shape:\n",
    "            return classify_sample(X)\n",
    "        elif len(X.shape) > 1 and X[0].shape == w.shape:\n",
    "            return classify_samples(X)\n",
    "        else:\n",
    "            raise Exception(\"Invalid test data shape. Either input an array (single sample) or a 2d array (multiple samples).\")\n",
    "\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our training data from a CSV using pandas\n",
    "df = pd.read_csv('./data/test-data/test_data.csv', encoding='utf8')\n",
    "\n",
    "# replace 0 by -1 in the label to conform to y being in {-1,1}\n",
    "df[\"success\"] = df[[\"success\"]].replace(0,-1)\n",
    "\n",
    "# Specify our X array by combining the training columns into a single 2D array.\n",
    "X = df[['age', 'interest']]\n",
    "# Grab the known y values\n",
    "y = df[[\"success\"]]\n",
    "\n",
    "# Convert pandas data frame ---> numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the following:\n",
    "# - maps classifiers values -1 or +1\n",
    "# - separates feature columns from the classifier column\n",
    "# - splits the data into training and testing sets\n",
    "# returns: xs_train, xs_test, ys_train, ys_test\n",
    "\n",
    "def prep_data(data, classifier_column_name, classifier_vals, train_size=0.75):\n",
    "\n",
    "    if len(classifier_vals) != 2:\n",
    "        raise ValueError(\"classifier_vals argument must be length 2 (binary classifier)\")\n",
    "    \n",
    "    # map each binary classifier value to either 1 or -1\n",
    "    data[classifier_column_name] = data[classifier_column_name].apply(lambda b: -1 if b == classifier_vals[0] else 1)\n",
    "\n",
    "    # separate the features from the classifications\n",
    "    colnames = data.columns.tolist()\n",
    "    feature_column_names = list(filter(lambda colname: colname != classifier_column_name, colnames))\n",
    "\n",
    "    xs = data[feature_column_names]\n",
    "    ys = data[classifier_column_name]\n",
    "\n",
    "    # split the data into training and testing data\n",
    "    datasets = train_test_split(xs, ys, train_size=train_size)\n",
    "\n",
    "    # map all the training data into numpy arrays\n",
    "    X_train, X_test, y_train, y_test = list(map(lambda s: s.to_numpy(), datasets))\n",
    "\n",
    "    # return the training and testing data\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.1051e+02 -1.8127e+04  5e+04  8e-01  4e-11\n",
      " 1: -4.4801e+02 -6.4436e+03  8e+03  1e-01  4e-11\n",
      " 2: -4.2254e+02 -1.6067e+03  1e+03  1e-02  4e-11\n",
      " 3: -5.1222e+02 -8.8012e+02  4e+02  3e-03  4e-11\n",
      " 4: -5.3297e+02 -8.2079e+02  3e+02  2e-03  4e-11\n",
      " 5: -5.5121e+02 -7.9470e+02  3e+02  1e-03  4e-11\n",
      " 6: -5.7925e+02 -7.3691e+02  2e+02  8e-04  5e-11\n",
      " 7: -6.0302e+02 -6.9006e+02  9e+01  3e-04  5e-11\n",
      " 8: -6.2249e+02 -6.6113e+02  4e+01  3e-05  6e-11\n",
      " 9: -6.3401e+02 -6.4663e+02  1e+01  1e-06  6e-11\n",
      "10: -6.3799e+02 -6.4074e+02  3e+00  1e-07  7e-11\n",
      "11: -6.3881e+02 -6.3979e+02  1e+00  5e-08  6e-11\n",
      "12: -6.3924e+02 -6.3928e+02  4e-02  7e-10  7e-11\n",
      "13: -6.3926e+02 -6.3926e+02  4e-04  7e-12  6e-11\n",
      "Optimal solution found.\n",
      "------------------- FROM OUR CALCULATIONS -----------------------\n",
      "Alphas =  [9.99999971e+00 9.99999597e+00 9.99999977e+00 9.99997703e+00\n",
      " 9.99999920e+00 9.99999586e+00 9.99999801e+00 9.99999756e+00\n",
      " 9.99999657e+00 9.99999828e+00 9.99999807e+00 9.99999890e+00\n",
      " 9.99999898e+00 9.99999487e+00 9.99999917e+00 9.99999960e+00\n",
      " 9.99999924e+00 9.99999865e+00 9.99999962e+00 9.99999659e+00\n",
      " 9.99999724e+00 9.99996851e+00 9.99999951e+00 9.99999478e+00\n",
      " 9.99999918e+00 9.99999913e+00 9.99999888e+00 9.99997534e+00\n",
      " 3.75652569e-04 9.99999889e+00 9.99999978e+00 5.41336864e+00\n",
      " 9.99999764e+00 4.22341497e+00 9.99999411e+00 9.99999562e+00\n",
      " 9.99999898e+00 9.99999951e+00 9.99999726e+00 9.99999786e+00\n",
      " 9.99999907e+00 9.63683523e+00 9.99999953e+00 9.99999898e+00\n",
      " 9.99999920e+00 9.99999916e+00 9.99999584e+00 9.99999965e+00\n",
      " 9.99999944e+00 9.99999703e+00 9.99999946e+00 9.99999925e+00\n",
      " 9.99999900e+00 9.99999970e+00 9.99999445e+00 9.99997540e+00\n",
      " 9.99999959e+00 9.99999770e+00 9.99999910e+00 9.99999945e+00\n",
      " 9.99999895e+00 9.99959906e+00 9.99999927e+00 9.99989543e+00\n",
      " 9.99999849e+00 9.99999921e+00]\n",
      "w =  [0.06945575 0.13783293]\n",
      "b =  [-7.072767849500479]\n",
      "------------------- FROM SVM CALCULATIONS -----------------------\n",
      "w =  [0.03487542 0.11762639]\n",
      "b =  [-5.63912864]\n"
     ]
    }
   ],
   "source": [
    "# Calculate our weight bias and alphas using our SVM function\n",
    "w, b, alphas = SVM(X_train, y_train)\n",
    "\n",
    "# Display results\n",
    "print(\"------------------- FROM OUR CALCULATIONS -----------------------\")\n",
    "print('Alphas = ',alphas[alphas > 1e-4])\n",
    "print('w = ', w)\n",
    "print('b = ', b)\n",
    "\n",
    "# Here, we look at the SVM calculations for a sanity check\n",
    "print(\"------------------- FROM SVM CALCULATIONS -----------------------\")\n",
    "clf = SVC_sklearn(C = 10, kernel = 'linear')\n",
    "clf.fit(X, y.ravel()) \n",
    "w_svm=clf.coef_[0]\n",
    "b_svm=clf.intercept_\n",
    "print(\"w = \",w_svm) \n",
    "print(\"b = \",b_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.1051e+02 -1.8127e+04  5e+04  8e-01  4e-11\n",
      " 1: -4.4801e+02 -6.4436e+03  8e+03  1e-01  4e-11\n",
      " 2: -4.2254e+02 -1.6067e+03  1e+03  1e-02  4e-11\n",
      " 3: -5.1222e+02 -8.8012e+02  4e+02  3e-03  4e-11\n",
      " 4: -5.3297e+02 -8.2079e+02  3e+02  2e-03  4e-11\n",
      " 5: -5.5121e+02 -7.9470e+02  3e+02  1e-03  4e-11\n",
      " 6: -5.7925e+02 -7.3691e+02  2e+02  8e-04  5e-11\n",
      " 7: -6.0302e+02 -6.9006e+02  9e+01  3e-04  5e-11\n",
      " 8: -6.2249e+02 -6.6113e+02  4e+01  3e-05  6e-11\n",
      " 9: -6.3401e+02 -6.4663e+02  1e+01  1e-06  6e-11\n",
      "10: -6.3799e+02 -6.4074e+02  3e+00  1e-07  7e-11\n",
      "11: -6.3881e+02 -6.3979e+02  1e+00  5e-08  6e-11\n",
      "12: -6.3924e+02 -6.3928e+02  4e-02  7e-10  7e-11\n",
      "13: -6.3926e+02 -6.3926e+02  4e-04  7e-12  6e-11\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1,  1,\n",
       "        1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1, -1,  1, -1,  1, -1,\n",
       "       -1, -1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
       "        1, -1, -1, -1, -1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1, -1, -1, -1])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVM_classifier(X_train, y_train)\n",
    "classifier(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    numtests = X_test.shape[0]\n",
    "    results = classifier(X_test) == y_test.flatten()\n",
    "    numcorrect = np.count_nonzero(results)\n",
    "    successrate = numcorrect / numtests\n",
    "    return successrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(clf.predict, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test[0][0]) == np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd90b7313dd068aed6076c54fbc0f4176173242dcb889e69e2c3450f3e374009"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
