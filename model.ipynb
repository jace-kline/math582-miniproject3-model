{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 582 Miniproject 3 - Model Development\n",
    "\n",
    "The purpose of this notebook is implment dual SVM convex quadratic optimization for the purposes of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel function definitions\n",
    "\n",
    "def polynomial_kernel (x_i, x_j, d):\n",
    "\t\"\"\"\n",
    "\tpolynomial_kernel generates a kernel for SVM in a polynomial format\n",
    "\n",
    "\tx_i: ith column to chose from\n",
    "\tx_j: jth row to choose from \n",
    "\td: polynomial degree\n",
    "\n",
    "\t:return: kernel values for the i, jth value\n",
    "\t\"\"\"\n",
    "\treturn (np.dot(x_i, x_j) + 1/2)**d\n",
    "\n",
    "def linear_kernel (x_i, x_j):\n",
    "\t\"\"\"\n",
    "\tlinear_kernel generates a kernel for SVM in a linear format\n",
    "\n",
    "\tx_i: ith column to chose from\n",
    "\tx_j: jth row to choose from \n",
    "\t:return: kernel values for the i, jth value\n",
    "\t\"\"\"\n",
    "\treturn (np.dot(x_i, x_j))\n",
    "\n",
    "def rbf_kernel (x_i, x_j,gamma=1):\n",
    "\t\"\"\"\n",
    "\trbf_kernel generates a kernel for SVM in a radial format\n",
    "\n",
    "\tx_i: ith column to chose from\n",
    "\tx_j: jth row to choose from \n",
    "\tgamma: defines how far the influence of a single training example reaches\n",
    "\t:return: kernel values for the i, jth value\n",
    "\t\"\"\"\n",
    "\treturn (np.exp(-gamma*(np.linalg.norm(x_j - x_i))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the kernel matrix from a kernel spec and data\n",
    "\n",
    "def build_k (X, kernel_type='linear_kernel', poly_power=3, gamma=1):\n",
    "\t\"\"\"\n",
    "\tbuild_k generates a kernel to use inside of an SVM calculation\n",
    "\tX: Training data for our calculations\n",
    "\tkernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "\tpoly_power: An optional parameter to define to what degree the polynomial should be calculated\n",
    "\tgamma: An optional parameter that defines how far the influence of a single training example reaches\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tN = X.shape[0]\n",
    "\tK = np.zeros((N, N))\n",
    "\tfor i in range(X.shape[0]):\n",
    "\t\tx_i = X[i]\n",
    "\t\tfor j in range(X.shape[0]):\n",
    "\t\t\tx_j = X[j]\n",
    "\n",
    "\t\t\tif kernel_type == 'linear_kernel':\n",
    "\t\t\t\tK[i][j] = linear_kernel(x_i, x_j)\n",
    "\n",
    "\t\t\telif kernel_type == 'polynomial_kernel':\n",
    "\t\t\t\tK[i][j] = polynomial_kernel(x_i, x_j, poly_power)\n",
    "\n",
    "\t\t\telif kernel_type == 'rbf_kernel':\n",
    "\t\t\t\tK[i][j] = rbf_kernel(x_i, x_j, gamma)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError('Use kernal type polynomial_kernel, linear_kernel or rbf_kernel') \n",
    "\n",
    "\treturn K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This outputs weights, biases, and lagrange multipliers of our SVM Classifier\n",
    "def SVM(X, y, kernel_type='linear_kernel', C=10):\n",
    "\t\"\"\"\n",
    "\tSVM will calculate the weight and bias using the SVM quadratic method (soft margin)\n",
    "\tX: Training data used for calculations \n",
    "\ty: results of training data\n",
    "\tkernel_type: Specifies the type of kernel to use: linear_kernel, polynomial_kernel, rbf_kernel\n",
    "\tC: Trades off misclassification of training examples against simplicity of the decision surface\n",
    "\t:return: weight, bias, and alphas matrix\n",
    "\n",
    "\tHelp used: https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work/353605#353605\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grabs shape of our training data\n",
    "\tm, _ = X.shape\n",
    "\n",
    "\t# Make sure y values are floats and within -1 == y == 1\n",
    "\ty = y.reshape(-1,1) * 1.\n",
    "\n",
    "\t# Calculate our kernel\n",
    "\tK = build_k(X, kernel_type=kernel_type)\n",
    "\n",
    "\t# Compute \n",
    "\tH = np.matmul(y,y.T) * K * 1.\n",
    "\n",
    "\t#Converting into cvxopt format - as previously\n",
    "\tP = cvxopt_matrix(H)\n",
    "\tq = cvxopt_matrix(-np.ones((m, 1)))\n",
    "\tG = cvxopt_matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "\th = cvxopt_matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "\tA = cvxopt_matrix(y.reshape(1, -1))\n",
    "\tb = cvxopt_matrix(np.zeros(1))\n",
    "\n",
    "\t#Run solver\n",
    "\tsol = cvxopt_solvers.qp(P, q, G, h, A, b)\n",
    "\talphas = np.array(sol['x'])\n",
    "\n",
    "\t# Calculating w, b\n",
    "\tw = ((y * alphas).T @ X).reshape(-1,1)\n",
    "\tS = (alphas > 1e-4).flatten()\n",
    "\n",
    "\tsv = X[S]\n",
    "\tsv_y = y[S]\n",
    "\talphas = alphas[S]\n",
    "\tb = sv_y - np.sum(build_k(sv) * alphas * sv_y, axis=0)\n",
    "\tb = [np.sum(b) / b.size]\n",
    "\n",
    "\treturn w, b, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def SVM_classifier(X_train, y_train, kernel, C):\n",
    "    w, b, alphas = SVM(X_train, y_train)\n",
    "\n",
    "    # classifies a single sample as +1 or -1\n",
    "    def classify_sample(x):\n",
    "        return (1 if np.inner(w, x) + b >= 0 else -1)\n",
    "\n",
    "    # classifies multiple samples as +1 or -1\n",
    "    def classify_samples(X_test):\n",
    "        return np.array(list(map(classify_sample, list(X_test))))\n",
    "\n",
    "    return classify_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our training data from a CSV using pandas\n",
    "df = pd.read_csv('./data/test-data/test_data.csv', encoding='utf8')\n",
    "\n",
    "# replace 0 by -1 in the label to conform to y being in {-1,1}\n",
    "df[\"success\"] = df[[\"success\"]].replace(0,-1)\n",
    "\n",
    "# Specify our X array by combining the training columns into a single 2D array.\n",
    "X = df[['age', 'interest']]\n",
    "# Grab the known y values\n",
    "y = df[[\"success\"]]\n",
    "\n",
    "# Convert pandas data frame ---> numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the following:\n",
    "# - maps classifiers values -1 or +1\n",
    "# - separates feature columns from the classifier column\n",
    "# - splits the data into training and testing sets\n",
    "# returns: xs_train, xs_test, ys_train, ys_test\n",
    "\n",
    "def prep_data(data, classifier_column_name, classifier_vals, train_size=0.75):\n",
    "\n",
    "    if len(classifier_vals) != 2:\n",
    "        raise ValueError(\"classifier_vals argument must be length 2 (binary classifier)\")\n",
    "    \n",
    "    # map each binary classifier value to either 1 or -1\n",
    "    data[classifier_column_name] = data[classifier_column_name].apply(lambda b: -1 if b == classifier_vals[0] else 1)\n",
    "\n",
    "    # separate the features from the classifications\n",
    "    colnames = data.columns.tolist()\n",
    "    feature_column_names = list(filter(lambda colname: colname != classifier_column_name, colnames))\n",
    "\n",
    "    xs = data[feature_column_names]\n",
    "    ys = data[classifier_column_name]\n",
    "\n",
    "    # split the data into training and testing data\n",
    "    datasets = train_test_split(xs, ys, train_size=train_size)\n",
    "\n",
    "    # map all the training data into numpy arrays\n",
    "    X_train, X_test, y_train, y_test = list(map(lambda s: s.to_numpy(), datasets))\n",
    "\n",
    "    # return the training and testing data\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate our weight bias and alphas using our SVM function\n",
    "w, b, alphas = SVM(X_train, y_train)\n",
    "\n",
    "# Display results\n",
    "print(\"------------------- FROM OUR CALCULATIONS -----------------------\")\n",
    "print('Alphas = ',alphas[alphas > 1e-4])\n",
    "print('w = ', w.flatten())\n",
    "print('b = ', b)\n",
    "\n",
    "# Here, we look at the SVM calculations for a sanity check\n",
    "print(\"------------------- FROM SVM CALCULATIONS -----------------------\")\n",
    "clf = SVC(C = 10, kernel = 'linear')\n",
    "clf.fit(X, y.ravel()) \n",
    "w_svm=clf.coef_[0]\n",
    "b_svm=clf.intercept_\n",
    "print(\"w = \",w_svm) \n",
    "print(\"b = \",b_svm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd90b7313dd068aed6076c54fbc0f4176173242dcb889e69e2c3450f3e374009"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
