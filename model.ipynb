{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 582 Miniproject 3 - Model Development\n",
    "\n",
    "The purpose of this notebook is implment dual SVM convex quadratic optimization for the purposes of binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from qpsolvers import solve_qp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>interest</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.657801</td>\n",
       "      <td>18.859917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.573729</td>\n",
       "      <td>17.969223</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.553424</td>\n",
       "      <td>29.463651</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.718035</td>\n",
       "      <td>25.704665</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.401919</td>\n",
       "      <td>16.770856</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>27.697220</td>\n",
       "      <td>18.799309</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>15.150959</td>\n",
       "      <td>72.000352</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>22.264378</td>\n",
       "      <td>68.453459</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>25.677420</td>\n",
       "      <td>90.118212</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>21.215594</td>\n",
       "      <td>48.265520</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age   interest  success\n",
       "0    23.657801  18.859917      0.0\n",
       "1    22.573729  17.969223      0.0\n",
       "2    32.553424  29.463651      0.0\n",
       "3     6.718035  25.704665      1.0\n",
       "4    14.401919  16.770856      0.0\n",
       "..         ...        ...      ...\n",
       "292  27.697220  18.799309      0.0\n",
       "293  15.150959  72.000352      1.0\n",
       "294  22.264378  68.453459      1.0\n",
       "295  25.677420  90.118212      1.0\n",
       "296  21.215594  48.265520      1.0\n",
       "\n",
       "[297 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('./data/test-data/test_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the following:\n",
    "# - maps classifiers values -1 or +1\n",
    "# - separates feature columns from the classifier column\n",
    "# - splits the data into training and testing sets\n",
    "# returns: xs_train, xs_test, ys_train, ys_test\n",
    "\n",
    "def prep_data(df, classifier_column_name, classifier_vals, train_size=0.75):\n",
    "\n",
    "    if len(classifier_vals) != 2:\n",
    "        raise ValueError(\"classifier_vals argument must be length 2 (binary classifier)\")\n",
    "    \n",
    "    # map each binary classifier value to either 1 or -1\n",
    "    df[classifier_column_name] = df[classifier_column_name].apply(lambda b: -1 if b == classifier_vals[0] else 1)\n",
    "\n",
    "    # separate the features from the classifications\n",
    "    colnames = df.columns.tolist()\n",
    "    feature_column_names = list(filter(lambda colname: colname != classifier_column_name, colnames))\n",
    "\n",
    "    xs = df[feature_column_names]\n",
    "    ys = df[classifier_column_name]\n",
    "\n",
    "    # split the data into training and testing data\n",
    "    datasets = train_test_split(xs, ys, train_size=train_size)\n",
    "\n",
    "    # map all the training data into numpy arrays\n",
    "    xs_train, xs_test, ys_train, ys_test = list(map(lambda s: s.to_numpy(), datasets))\n",
    "\n",
    "    # return the training and testing data\n",
    "    return xs_train, xs_test, ys_train, ys_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the kernel matrix K\n",
    "def gram_matrix(xs_train, k):\n",
    "    N = xs_train.shape[0]\n",
    "    K = np.zeros(shape=(N,N))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, i + 1):\n",
    "            K[i][j] = K[j][i] = k(xs_train[i], xs_train[j])\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_neg_eigvals(A):\n",
    "    eigvals = np.linalg.eigvals(A)\n",
    "    neg_eigvals = np.extract(eigvals < 0, eigvals)\n",
    "    print(\"Found {} negative eigenvalues.\\nNegative eigenvalues = {}\".format(len(neg_eigvals), neg_eigvals))\n",
    "\n",
    "# assume A is an NxN matrix\n",
    "def make_positive_definite(A):\n",
    "    N = A.shape[0]\n",
    "    # if all eigenvals are not > 0, then add perturbation and try again\n",
    "    while not np.all(np.linalg.eigvals(A) > 0):\n",
    "        # print_neg_eigvals(A)\n",
    "        epsilon = 1e-12\n",
    "        perturbation = epsilon * np.identity(N)\n",
    "        A += perturbation\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function used to get the optimal lagrange multipliers (alpha) for given training data, kernel function, and cost function\n",
    "def lagrange_multipliers(xs_train, ys_train, k, C):\n",
    "    # Solves for x in the following...\n",
    "    # min 1/2 x^T P x + q^T x\n",
    "    # s.t.\n",
    "    #  Gx <= h\n",
    "\n",
    "    N = xs_train.shape[0]\n",
    "\n",
    "    # compute the entries in the kernel matrix\n",
    "    K = gram_matrix(xs_train, k)\n",
    "    # Y = np.diag(ys_train)\n",
    "\n",
    "    # quadratic program parameters\n",
    "    P = make_positive_definite(np.outer(ys_train,ys_train) * K)\n",
    "    q = -1 * np.ones(N)\n",
    "    G = np.vstack([ys_train, -1 * ys_train, -1 * np.identity(N), np.identity(N)])\n",
    "    h = np.concatenate([np.zeros((N+2)), C * np.ones((N))])\n",
    "\n",
    "    solution = solve_qp(P, q, G, h)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, xs_test, ys_test):\n",
    "    numtests = xs_test.shape[0]\n",
    "    results = classifier(xs_test) == ys_test\n",
    "    numcorrect = len(list(filter(lambda b: b, list(results))))\n",
    "    successrate = numcorrect / numtests\n",
    "    return successrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_svm_classifier(xs_train, ys_train, k, C):\n",
    "\n",
    "    # anything less than this value is considered by us to be 0 when selecting support vectors\n",
    "    MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5\n",
    "\n",
    "    N = xs_train.shape[0]\n",
    "    alpha = lagrange_multipliers(xs_train, ys_train, k, C)\n",
    "\n",
    "    # select support multipliers that are > 0\n",
    "    support_vector_indices = (alpha > MIN_SUPPORT_VECTOR_MULTIPLIER)\n",
    "    support_multipliers = alpha[support_vector_indices]\n",
    "    support_vectors = xs_train[support_vector_indices]\n",
    "    support_classifications = ys_train[support_vector_indices]\n",
    "\n",
    "    fw = lambda n: support_multipliers[n] * support_classifications[n] * support_vectors[n]\n",
    "    w = np.array(list(reduce(lambda v1, v2: v1 + v2, [fw(n) for n in range(0, len(support_vectors))])))\n",
    "\n",
    "    # select support multipliers that are > 0 AND < C (the cost)\n",
    "    within_cost_indices = (support_multipliers < C)\n",
    "    support_multipliers_ = support_multipliers[within_cost_indices]\n",
    "    support_vectors_ = support_vectors[within_cost_indices]\n",
    "    support_classifications_ = support_classifications[within_cost_indices]\n",
    "\n",
    "    fb = lambda n: np.abs(support_classifications_[n] - np.inner(w, support_vectors_[n]))\n",
    "    b = np.median(np.array([fb(n) for n in range(0, len(support_vectors_))]))\n",
    "\n",
    "    # builds the classifier function\n",
    "    # choose whether to invert the classifications of +1 / -1\n",
    "    def classifier(invert=False):\n",
    "        # classifies a single sample as +1 or -1\n",
    "        def classify_sample(x_test):\n",
    "            v = -1 if invert else 1\n",
    "            return (v if np.inner(w, x_test) + b >= 0 else -1 * v)\n",
    "\n",
    "        # classifies multiple samples as +1 or -1\n",
    "        def classify_samples(xs_test):\n",
    "            return np.array(list(map(classify_sample, list(xs_test))))\n",
    "\n",
    "        return classify_samples\n",
    "\n",
    "    # figure out whether to invert the classification identification (+1 or -1)\n",
    "    # based on training data\n",
    "    # invert if the success rate is less than 50% on training data\n",
    "    invert = evaluate_classifier(classifier(invert=False), xs_train, ys_train) < 0.5\n",
    "\n",
    "    # take 5 samples from \n",
    "\n",
    "    return classifier(invert=invert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel functions\n",
    "\n",
    "def linear_kernel():\n",
    "    return np.inner\n",
    "\n",
    "def poly_kernel(dim, offset=1.0):\n",
    "    if dim < 1:\n",
    "        raise ValueError(\"Invalid polynomial dimension for polynomial kernel\")\n",
    "\n",
    "    def f(x, y):\n",
    "        return (offset + np.inner(x, y)) ** dim\n",
    "\n",
    "    return f\n",
    "\n",
    "def gaussian_kernel(sigma):\n",
    "    def f(x, y):\n",
    "        exponent = -np.sqrt(np.linalg.norm(x-y) ** 2 / (2 * sigma ** 2))\n",
    "        return np.exp(exponent)\n",
    "    return f\n",
    "\n",
    "def hyperbolic_tangent_kernel(kappa, c):\n",
    "    def f(x, y):\n",
    "        return np.tanh(kappa * np.dot(x, y) + c)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train, xs_test, ys_train, ys_test = prep_data(df, \"success\", [0.0, 1.0], train_size=0.75)\n",
    "\n",
    "k = linear_kernel()\n",
    "C = 1\n",
    "classifier = build_svm_classifier(xs_train, ys_train, k, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classifier(classifier, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8266666666666667"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linsvc = SVC(kernel=k, C=C)\n",
    "linsvc.fit(xs_train, ys_train)\n",
    "evaluate_classifier(linsvc.predict, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True,  True,  True, False, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(xs_test) == ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True,  True,  True, False, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linsvc.predict(xs_test) == ys_test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "779c798a75e127e3aa96660aebf9b74d3e571412428da99918dfc4cadf485d44"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
